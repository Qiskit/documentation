---
title: ADAM
description: API reference for qiskit.aqua.components.optimizers.ADAM
in_page_toc_min_heading_level: 1
python_api_type: class
python_api_name: qiskit.aqua.components.optimizers.ADAM
---

# ADAM

<Class id="qiskit.aqua.components.optimizers.ADAM" isDedicatedPage={true} github="https://github.com/qiskit-community/qiskit-aqua/tree/stable/0.7/qiskit/aqua/components/optimizers/adam_amsgrad.py" signature="ADAM(maxiter=10000, tol=1e-06, lr=0.001, beta_1=0.9, beta_2=0.99, noise_factor=1e-08, eps=1e-10, amsgrad=False, snapshot_dir=None)" modifiers="class">
  Adam and AMSGRAD optimizer.

  **Adam**

  *Kingma, Diederik & Ba, Jimmy. (2014).*

  Adam: A Method for Stochastic Optimization. International Conference on Learning Representations.

  Adam is a gradient-based optimization algorithm that is relies on adaptive estimates of lower-order moments. The algorithm requires little memory and is invariant to diagonal rescaling of the gradients. Furthermore, it is able to cope with non-stationary objective functions and noisy and/or sparse gradients.

  **AMSGRAD**

  *Sashank J. Reddi and Satyen Kale and Sanjiv Kumar. (2018).*

  On the Convergence of Adam and Beyond. International Conference on Learning Representations.

  AMSGRAD (a variant of ADAM) uses a ‘long-term memory’ of past gradients and, thereby, improves convergence properties.

  **Parameters**

  *   **maxiter** (`int`) – Maximum number of iterations
  *   **tol** (`float`) – Tolerance for termination
  *   **lr** (`float`) – Value >= 0, Learning rate.
  *   **beta\_1** (`float`) – Value in range 0 to 1, Generally close to 1.
  *   **beta\_2** (`float`) – Value in range 0 to 1, Generally close to 1.
  *   **noise\_factor** (`float`) – Value >= 0, Noise factor
  *   **eps** (`float`) – Value >=0, Epsilon to be used for finite differences if no analytic gradient method is given.
  *   **amsgrad** (`bool`) – True to use AMSGRAD, False if not
  *   **snapshot\_dir** (`Optional`\[`str`]) – If not None save the optimizer’s parameter after every step to the given directory

  ## Attributes

  <span id="adam-bounds-support-level" />

  ### bounds\_support\_level

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.bounds_support_level">
    Returns bounds support level
  </Attribute>

  <span id="adam-gradient-support-level" />

  ### gradient\_support\_level

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.gradient_support_level">
    Returns gradient support level
  </Attribute>

  <span id="adam-initial-point-support-level" />

  ### initial\_point\_support\_level

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.initial_point_support_level">
    Returns initial point support level
  </Attribute>

  <span id="adam-is-bounds-ignored" />

  ### is\_bounds\_ignored

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_bounds_ignored">
    Returns is bounds ignored
  </Attribute>

  <span id="adam-is-bounds-required" />

  ### is\_bounds\_required

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_bounds_required">
    Returns is bounds required
  </Attribute>

  <span id="adam-is-bounds-supported" />

  ### is\_bounds\_supported

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_bounds_supported">
    Returns is bounds supported
  </Attribute>

  <span id="adam-is-gradient-ignored" />

  ### is\_gradient\_ignored

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_gradient_ignored">
    Returns is gradient ignored
  </Attribute>

  <span id="adam-is-gradient-required" />

  ### is\_gradient\_required

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_gradient_required">
    Returns is gradient required
  </Attribute>

  <span id="adam-is-gradient-supported" />

  ### is\_gradient\_supported

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_gradient_supported">
    Returns is gradient supported
  </Attribute>

  <span id="adam-is-initial-point-ignored" />

  ### is\_initial\_point\_ignored

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_initial_point_ignored">
    Returns is initial point ignored
  </Attribute>

  <span id="adam-is-initial-point-required" />

  ### is\_initial\_point\_required

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_initial_point_required">
    Returns is initial point required
  </Attribute>

  <span id="adam-is-initial-point-supported" />

  ### is\_initial\_point\_supported

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.is_initial_point_supported">
    Returns is initial point supported
  </Attribute>

  <span id="adam-setting" />

  ### setting

  <Attribute id="qiskit.aqua.components.optimizers.ADAM.setting">
    Return setting
  </Attribute>

  ## Methods

  <span id="adam-get-support-level" />

  ### get\_support\_level

  <Function id="qiskit.aqua.components.optimizers.ADAM.get_support_level" signature="ADAM.get_support_level()">
    Return support level dictionary
  </Function>

  <span id="adam-gradient-num-diff" />

  ### gradient\_num\_diff

  <Function id="qiskit.aqua.components.optimizers.ADAM.gradient_num_diff" signature="ADAM.gradient_num_diff(x_center, f, epsilon, max_evals_grouped=1)" modifiers="static">
    We compute the gradient with the numeric differentiation in the parallel way, around the point x\_center.

    **Parameters**

    *   **x\_center** (*ndarray*) – point around which we compute the gradient
    *   **f** (*func*) – the function of which the gradient is to be computed.
    *   **epsilon** (*float*) – the epsilon used in the numeric differentiation.
    *   **max\_evals\_grouped** (*int*) – max evals grouped

    **Returns**

    the gradient computed

    **Return type**

    grad
  </Function>

  <span id="adam-load-params" />

  ### load\_params

  <Function id="qiskit.aqua.components.optimizers.ADAM.load_params" signature="ADAM.load_params(load_dir)">
    load params
  </Function>

  <span id="adam-minimize" />

  ### minimize

  <Function id="qiskit.aqua.components.optimizers.ADAM.minimize" signature="ADAM.minimize(objective_function, initial_point, gradient_function)" />

  <span id="adam-optimize" />

  ### optimize

  <Function id="qiskit.aqua.components.optimizers.ADAM.optimize" signature="ADAM.optimize(num_vars, objective_function, gradient_function=None, variable_bounds=None, initial_point=None)">
    Perform optimization.

    **Parameters**

    *   **num\_vars** (*int*) – number of parameters to be optimized.
    *   **objective\_function** (*callable*) – handle to a function that computes the objective function.
    *   **gradient\_function** (*callable*) – handle to a function that computes the gradient of the objective function, or None if not available.
    *   **variable\_bounds** (*list\[(float, float)]*) – deprecated
    *   **initial\_point** (*numpy.ndarray\[float]*) – initial point.

    **Returns**

    tuple has (point, value, nfev) where

    > point: is a 1D numpy.ndarray\[float] containing the solution
    >
    > value: is a float with the objective function value
    >
    > nfev: number of objective function calls made if available or None

    **Return type**

    tuple(numpy.ndarray, float, int)
  </Function>

  <span id="adam-print-options" />

  ### print\_options

  <Function id="qiskit.aqua.components.optimizers.ADAM.print_options" signature="ADAM.print_options()">
    Print algorithm-specific options.
  </Function>

  <span id="adam-save-params" />

  ### save\_params

  <Function id="qiskit.aqua.components.optimizers.ADAM.save_params" signature="ADAM.save_params(snapshot_dir)">
    save params
  </Function>

  <span id="adam-set-max-evals-grouped" />

  ### set\_max\_evals\_grouped

  <Function id="qiskit.aqua.components.optimizers.ADAM.set_max_evals_grouped" signature="ADAM.set_max_evals_grouped(limit)">
    Set max evals grouped
  </Function>

  <span id="adam-set-options" />

  ### set\_options

  <Function id="qiskit.aqua.components.optimizers.ADAM.set_options" signature="ADAM.set_options(**kwargs)">
    Sets or updates values in the options dictionary.

    The options dictionary may be used internally by a given optimizer to pass additional optional values for the underlying optimizer/optimization function used. The options dictionary may be initially populated with a set of key/values when the given optimizer is constructed.

    **Parameters**

    **kwargs** (*dict*) – options, given as name=value.
  </Function>

  <span id="adam-wrap-function" />

  ### wrap\_function

  <Function id="qiskit.aqua.components.optimizers.ADAM.wrap_function" signature="ADAM.wrap_function(function, args)" modifiers="static">
    Wrap the function to implicitly inject the args at the call of the function.

    **Parameters**

    *   **function** (*func*) – the target function
    *   **args** (*tuple*) – the args to be injected

    **Returns**

    wrapper

    **Return type**

    function\_wrapper
  </Function>
</Class>


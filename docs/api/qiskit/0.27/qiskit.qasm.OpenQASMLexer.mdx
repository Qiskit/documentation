---
title: OpenQASMLexer
description: API reference for qiskit.qasm.OpenQASMLexer
in_page_toc_min_heading_level: 1
python_api_type: class
python_api_name: qiskit.qasm.OpenQASMLexer
---

# qiskit.qasm.OpenQASMLexer

<Class id="qiskit.qasm.OpenQASMLexer" isDedicatedPage={true} github="https://github.com/qiskit/qiskit/tree/stable/0.17/qiskit/qasm/pygments/lexer.py" signature="OpenQASMLexer(*args, **kwds)" modifiers="class">
  A pygments lexer for OpenQasm.

  ### \_\_init\_\_

  <Function id="qiskit.qasm.OpenQASMLexer.__init__" signature="__init__(**options)">
    Initialize self. See help(type(self)) for accurate signature.
  </Function>

  ## Methods

  |                                                                                                                                                  |                                                                                                   |
  | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------- |
  | [`__init__`](#qiskit.qasm.OpenQASMLexer.__init__ "qiskit.qasm.OpenQASMLexer.__init__")(\*\*options)                                              | Initialize self.                                                                                  |
  | [`add_filter`](#qiskit.qasm.OpenQASMLexer.add_filter "qiskit.qasm.OpenQASMLexer.add_filter")(filter\_, \*\*options)                              | Add a new stream filter to this lexer.                                                            |
  | [`analyse_text`](#qiskit.qasm.OpenQASMLexer.analyse_text "qiskit.qasm.OpenQASMLexer.analyse_text")(text)                                         | Has to return a float between `0` and `1` that indicates if a lexer wants to highlight this text. |
  | [`get_tokens`](#qiskit.qasm.OpenQASMLexer.get_tokens "qiskit.qasm.OpenQASMLexer.get_tokens")(text\[, unfiltered])                                | Return an iterable of (tokentype, value) pairs generated from text.                               |
  | [`get_tokens_unprocessed`](#qiskit.qasm.OpenQASMLexer.get_tokens_unprocessed "qiskit.qasm.OpenQASMLexer.get_tokens_unprocessed")(text\[, stack]) | Split `text` into (tokentype, text) pairs.                                                        |

  ## Attributes

  |                   |   |
  | ----------------- | - |
  | `alias_filenames` |   |
  | `aliases`         |   |
  | `filenames`       |   |
  | `flags`           |   |
  | `gates`           |   |
  | `mimetypes`       |   |
  | `name`            |   |
  | `priority`        |   |
  | `tokens`          |   |

  ### add\_filter

  <Function id="qiskit.qasm.OpenQASMLexer.add_filter" signature="add_filter(filter_, **options)">
    Add a new stream filter to this lexer.
  </Function>

  ### analyse\_text

  <Function id="qiskit.qasm.OpenQASMLexer.analyse_text" signature="analyse_text(text)" modifiers="static">
    Has to return a float between `0` and `1` that indicates if a lexer wants to highlight this text. Used by `guess_lexer`. If this method returns `0` it won’t highlight it in any case, if it returns `1` highlighting with this lexer is guaranteed.

    The LexerMeta metaclass automatically wraps this function so that it works like a static method (no `self` or `cls` parameter) and the return value is automatically converted to float. If the return value is an object that is boolean False it’s the same as if the return values was `0.0`.
  </Function>

  ### get\_tokens

  <Function id="qiskit.qasm.OpenQASMLexer.get_tokens" signature="get_tokens(text, unfiltered=False)">
    Return an iterable of (tokentype, value) pairs generated from text. If unfiltered is set to True, the filtering mechanism is bypassed even if filters are defined.

    Also preprocess the text, i.e. expand tabs and strip it if wanted and applies registered filters.
  </Function>

  ### get\_tokens\_unprocessed

  <Function id="qiskit.qasm.OpenQASMLexer.get_tokens_unprocessed" signature="get_tokens_unprocessed(text, stack=('root'))">
    Split `text` into (tokentype, text) pairs.

    `stack` is the inital stack (default: `['root']`)
  </Function>
</Class>


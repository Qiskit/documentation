{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe74e692",
   "metadata": {},
   "source": [
    "{/* cspell:ignore hyperparameters */}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde95705",
   "metadata": {},
   "source": [
    "# Singularity Machine Learning - Classification: A Qiskit Function by Multiverse Computing\n",
    "\n",
    "<LegacyContent>\n",
    "<Admonition type=\"note\">\n",
    "This documentation is relevant to IBM Quantum&reg; Platform Classic. If you need the newer version, go to the new [IBM Quantum Platform documentation.](https://quantum.cloud.ibm.com/docs/guides/multiverse-computing-singularity)\n",
    "</Admonition>\n",
    "</LegacyContent>\n",
    "<CloudContent>\n",
    "<Admonition type=\"note\">\n",
    "This documentation is relevant to the new IBM Quantum&reg; Platform. If you need the previous version, return to the [IBM Quantum Platform Classic documentation.](https://docs.quantum.ibm.com/guides/multiverse-computing-singularity)\n",
    "</Admonition>\n",
    "</CloudContent>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db0bb4",
   "metadata": {
    "tags": [
     "version-info"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9985b8f4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "With the \"Singularity Machine Learning - Classification\" function, you can solve real-world machine learning problems on quantum hardware without requiring quantum expertise. This Application function, based on ensemble methods, is a hybrid classifier. It leverages classical methods like boosting, bagging, and stacking for initial ensemble training. Subsequently, quantum algorithms such as variational quantum eigensolver (VQE) and quantum approximate optimization algorithm (QAOA) are employed to enhance the trained ensemble's diversity, generalization capabilities, and overall complexity.\n",
    "\n",
    "Unlike other quantum machine learning solutions, this function is capable of handling large-scale datasets with millions of examples and features without being limited by the number of qubits in the target QPU. The number of qubits only determines the size of the ensemble that can be trained. It is also highly flexible, and can be used to solve classification problems across a wide range of domains, including finance, healthcare, and cybersecurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e72a4",
   "metadata": {},
   "source": [
    "It consistently achieves high accuracies on classically challenging problems involving high-dimensional, noisy, and imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c5737",
   "metadata": {},
   "source": [
    "![How it works](/images/guides/multiverse-computing-singularity/how_it_works.avif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639e5e5",
   "metadata": {},
   "source": [
    "It is built for:\n",
    "1. Engineers and data scientists at companies seeking to enhance their tech offerings by integrating quantum machine learning into their products and services,\n",
    "2. Researchers at quantum research labs exploring quantum machine learning applications and looking to leverage quantum computing for classification tasks, and\n",
    "3. Students and teachers at educational institutions in courses like machine learning, and who are looking to demonstrate the advantages of quantum computing.\n",
    "\n",
    "The following example showcases its various functionalities, including `create`, `list`, `fit`, and `predict`, and demonstrates its usage in a synthetic problem comprising two interleaving half circles, a notoriously challenging problem due to its nonlinear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf056e3",
   "metadata": {},
   "source": [
    "{/* cspell:ignore quantumly */}\n",
    "\n",
    "## Function description\n",
    "\n",
    "This Qiskit Function allows users to solve binary classification problems using Singularity's quantum-enhanced ensemble classifier. Behind the scenes, it uses a hybrid approach to classically train an ensemble of classifiers on the labeled dataset, and then optimize it for maximum diversity and generalization using the Quantum Approximate Optimization Algorithm (QAOA) on IBM&reg; QPUs. Through a user-friendly interface, users can configure a classifier according to their requirements, train it on the dataset of their choice, and use it to make predictions on a previously unseen dataset.\n",
    "\n",
    "To solve a generic classification problem:\n",
    "1. Preprocess the dataset, and split it into training and testing sets. Optionally, you can further split the training set into training and validation sets. This can be achieved using [scikit-learn](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "2. If the training set is imbalanced, you can resample it to balance the classes using [imbalanced-learn](https://imbalanced-learn.org/stable/introduction.html#problem-statement-regarding-imbalanced-data-sets).\n",
    "3. Upload the training, validation, and test sets separately to the function's storage using the catalog's `file_upload` method, passing it the relevant path each time.\n",
    "4. Initialize the quantum classifier by using the function's `create` action, which accepts hyperparameters such as the number and types of learners, the regularization (lambda value), and optimization options including the number of layers, the type of classical optimizer, the quantum backend, and so on.\n",
    "5. Train the quantum classifier on the training set using the function's `fit` action, passing it the labeled training set, and the validation set if applicable.\n",
    "6. Make predictions on the previously unseen test set using the function's `predict` action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2817b13",
   "metadata": {},
   "source": [
    "## Action-based approach\n",
    "\n",
    "The function uses an action-based approach. You can think of it as a virtual environment where you use actions to perform tasks or change its state. Currently, it offers the following actions: [list](#1-list), [create](#2-create), [delete](#3-delete), [fit](#4-fit), [predict](#5-predict), [fit_predict](#6-fit-predict), and [create_fit_predict](#7-create-fit-predict). The following example demonstrates the `create_fit_predict` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7d2edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status:  QUEUED\n",
      "Action result status:  ok\n",
      "Action result message:  Classifier created, fitted, and predicted.\n",
      "Predictions (first five results):  [0, 1, 0, 0, 0]\n",
      "Probabilities (first five results):  [[0.7975136940667589, 0.20248630593324138], [0.08459026150938889, 0.9154097384906114], [0.8043402543461567, 0.19565974565384334], [0.7975136940667589, 0.20248630593324138], [0.7975136940667589, 0.20248630593324138]]\n"
     ]
    }
   ],
   "source": [
    "# Import QiskitFunctionsCatalog to load the\n",
    "# \"Singularity Machine Learning - Classification\" function by Multiverse Computing\n",
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "\n",
    "# Import the make_moons and the train_test_split functions from scikit-learn\n",
    "# to create a synthetic dataset and split it into training and test datasets\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# authentication\n",
    "# If you have not previously saved your credentials, follow instructions at\n",
    "# https://docs.quantum.ibm.com/guides/functions\n",
    "# to authenticate with your API key.\n",
    "catalog = QiskitFunctionsCatalog()\n",
    "\n",
    "# load \"Singularity Machine Learning - Classification\" function by Multiverse Computing\n",
    "singularity = catalog.load(\"multiverse/singularity\")\n",
    "\n",
    "# generate the synthetic dataset\n",
    "X, y = make_moons(n_samples=1000)\n",
    "\n",
    "# split the data into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "job = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    num_learners=10,\n",
    "    regularization=0.01,\n",
    "    optimizer_options={\"simulator\": True},\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    options={\"save\": False},\n",
    ")\n",
    "\n",
    "# get job status and result\n",
    "status = job.status()\n",
    "result = job.result()\n",
    "\n",
    "print(\"Job status: \", status)\n",
    "print(\"Action result status: \", result[\"status\"])\n",
    "print(\"Action result message: \", result[\"message\"])\n",
    "print(\"Predictions (first five results): \", result[\"data\"][\"predictions\"][:5])\n",
    "print(\n",
    "    \"Probabilities (first five results): \",\n",
    "    result[\"data\"][\"probabilities\"][:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5303ffb",
   "metadata": {},
   "source": [
    "### 1. List\n",
    "\n",
    "The `list` action retrieves all stored classifiers in `*.pkl.tar` format from the shared data directory. You can also access the contents of this directory by using the `catalog.files()` method. In general, the list action searches for files with the `*.pkl.tar` extension in the shared data directory and returns them in a list format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb844f8",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a52919",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(action=\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d0fca",
   "metadata": {},
   "source": [
    "### 2. Create\n",
    "\n",
    "The `create` action creates a classifier of the specified `quantum_classifier` type by using the provided parameters, and saves it in the shared data directory.\n",
    "\n",
    "<Admonition type=\"note\">\n",
    "The function currently supports only the `QuantumEnhancedEnsembleClassifier`.\n",
    "</Admonition>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369da086",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "| name | string | The name of the quantum classifier, e.g., \"spam_classifier\". | Yes |\n",
    "| quantum_classifier | string | The type of the quantum classifier, i.e., \"QuantumEnhancedEnsembleClassifier\". Default is \"QuantumEnhancedEnsembleClassifier\". | No |\n",
    "| num_learners | integer | The number of learners in the ensemble. Default is set to a small value of 10 to facilitate quick training and optimization. | No |\n",
    "| learners_types | list | Types of learners. Among supported types are: `DecisionTreeClassifier`, `GaussianNB`, `KNeighborsClassifier`, `MLPClassifier`, and `LogisticRegression`. Further details related to each can be found in the [scikit-learn documentation](https://scikit-learn.org/stable/supervised_learning.html). Default is [`DecisionTreeClassifier`]. | No |\n",
    "| learners_proportions | list | Proportions of each learner type in the ensemble. Default is: `[1.0]`. | No |\n",
    "| learners_options | list | Options for each learner type in the ensemble. For a complete list of options corresponding to the chosen learner type/s, consult [scikit-learn documentation](https://scikit-learn.org/stable/supervised_learning.html). Default is `[{\"max_depth\": 3, \"splitter\": \"random\", \"class_weight\": None}]`. | No |\n",
    "| regularization | float | Regularization parameter. Default is 0.01. | No |\n",
    "| weight_update_method | string | Method for update of sample weights from among \"logarithmic\" and \"quadratic\". Default is \"logarithmic\". | No |\n",
    "| sample_scaling | boolean | Whether sample scaling should be applied. Default is False. | No |\n",
    "| prediction_scaling | float | Scaling factor for predictions. Default is None. | No |\n",
    "| optimizer_options | dictionary | QAOA optimizer options. A list of available options is presented later in this documentation. | No |\n",
    "| voting | string | Use majority voting (\"hard\") or average of probabilities (\"soft\") for aggregating learners' predictions/probabilities. Default is \"hard\". | No |\n",
    "| prob_threshold | float | Optimal probability threshold. Default is 0.5. | No |\n",
    "| instance | string | IBM instance. Default is None. | No |\n",
    "| backend_name | string | IBM compute resource. Default is None, which means the backend with the fewest pending jobs will be used. | No |\n",
    "| random_state | integer | Control randomness for repeatability. Default is None. | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56381c3e",
   "metadata": {},
   "source": [
    "{/* cspell:ignore sparsify, sparsification */}\n",
    "\n",
    "- Additionally, `optimizer_options` are enlisted as follows:\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| num_solutions | integer | The number of solutions. Default is 1024. | No |\n",
    "| reps | integer | The number of repetitions. Default is 4. | No |\n",
    "| sparsify | float | The sparsification threshold. Default is 0.001. | No |\n",
    "| theta | float | The initial value of theta, a variational parameter of QAOA. Default is None. | No |\n",
    "| simulator | boolean | Whether to use a simulator or a QPU. Default is False. | No |\n",
    "| classical_optimizer | string | Name of the classical optimizer for the QAOA. Default is \"COBYLA\". All solvers offered by SciPy, as enlisted [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize), are usable. You will need to set `classical_optimizer_options` accordingly. | No |\n",
    "| classical_optimizer_options | dictionary | Classical optimizer options. For a complete list of available options, consult [SciPy documentation](https://docs.scipy.org/doc/scipy/reference/). Default is `{\"maxiter\": 60}`. | No |\n",
    "| optimization_level | integer | The depth of the QAOA circuit. Default is 3. | No |\n",
    "| num_transpiler_runs | integer | Number of transpiler runs. Default is 30. | No |\n",
    "| pass_manager_options | dictionary | Options for generating preset pass manager. Default is `{\"approximation_degree\": 1.0}`. | No |\n",
    "| estimator_options | dictionary | Estimator options. For a complete list of available options, consult [Qiskit Runtime Client documentation](/api/qiskit-ibm-runtime/options-estimator-options). | No |\n",
    "| sampler_options | dictionary | Sampler options. For a complete list of available options, consult the [Qiskit Runtime Client documentation](/api/qiskit-ibm-runtime/options-sampler-options). | No |\n",
    "\n",
    "- Default `estimator_options` are:\n",
    "\n",
    "|     Name    |    Type     | Value  |\n",
    "|-------------|-------------|-------------|\n",
    "| default_shots | integer | 1024 |\n",
    "| resilience_level | integer | 2 |\n",
    "| twirling | dictionary | `{\"enable_gates\": True}` |\n",
    "| dynamical_decoupling | dictionary | `{\"enable\": True}` |\n",
    "| resilience_options | dictionary | `{\"zne_mitigation\": False, \"zne\": {\"amplifier\": \"pea\", \"noise_factors\": [1.0, 1.3, 1.6], \"extrapolator\": [\"linear\", \"polynomial_degree_2\", \"exponential\"],}}` |\n",
    "\n",
    "- Default `sampler_options` are:\n",
    "\n",
    "|     Name    |    Type     | Value |\n",
    "|-------------|-------------|-------------|\n",
    "| twirling | dictionary | `{\"enable_gates\": True}` |\n",
    "| dynamical_decoupling | dictionary | `{\"enable\": True}` |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d370303",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(\n",
    "    action=\"create\",\n",
    "    name=\"classifier_name\",  # specify your custom name for the classifier here\n",
    "    num_learners=10,\n",
    "    regularization=0.01,\n",
    "    optimizer_options={\"simulator\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c6d82",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "\n",
    "- `name`:\n",
    "    - The name must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - No classifier with the same name should already exist in the shared data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a82e18",
   "metadata": {},
   "source": [
    "### 3. Delete\n",
    "\n",
    "The `delete` action removes a classifier from the shared data directory.\n",
    "\n",
    "#### Inputs\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "| name | string | The name of the classifier to delete. | Yes |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(\n",
    "    action=\"delete\",\n",
    "    name=\"classifier_name\",  # specify the name of the classifier to delete here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d12cc",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "\n",
    "- `name`:\n",
    "    - The name must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - A classifier with the same name should already exist in the shared data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2d434",
   "metadata": {},
   "source": [
    "### 4. Fit\n",
    "\n",
    "The `fit` action trains a classifier using the provided training data.\n",
    "\n",
    "#### Inputs\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "| name | string | The name of the classifier to train. | Yes |\n",
    "| X | array or list or string | The training data. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| y | array or list or string | The training target values. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| fit_params | dictionary | Additional parameters to pass to the `fit` method of the classifier. | No |\n",
    "\n",
    "##### fit_params\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| validation_data | tuple | The validation data and labels. | No |\n",
    "| pos_label | integer or string | The class label to be mapped to 1. | No |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(\n",
    "    action=\"fit\",\n",
    "    name=\"classifier_name\",  # specify the name of the classifier to train here\n",
    "    X=X_train,  # or \"X_train.npy\" if you uploaded it in the shared data directory\n",
    "    y=y_train,  # or \"y_train.npy\" if you uploaded it in the shared data directory\n",
    "    fit_params={},  # define the fit parameters here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daddb08",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "\n",
    "- `name`:\n",
    "    - The name must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - A classifier with the same name should already exist in the shared data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69856742",
   "metadata": {},
   "source": [
    "### 5. Predict\n",
    "\n",
    "The `predict` action is used to obtain hard and soft predictions (probabilities).\n",
    "\n",
    "#### Inputs\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "| name | string | The name of the classifier to be used. | Yes |\n",
    "| X | array or list or string | The test data. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| options[\"out\"] | string | The output JSON filename to save the predictions in the shared data directory. If not provided, the predictions are returned in the job result. | No |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(\n",
    "    action=\"predict\",\n",
    "    name=\"classifier_name\",  # specify the name of the classifier to use here\n",
    "    X=X_test,  # or \"X_test.npy\" if you uploaded it to the shared data directory\n",
    "    options={\n",
    "        \"out\": \"output.json\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe815dbd",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "\n",
    "- `name`:\n",
    "    - The name must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - A classifier with the same name should already exist in the shared data directory.\n",
    "- `options[\"out\"]`:\n",
    "    - The filename must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - It must have the `.json` extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ffba3",
   "metadata": {},
   "source": [
    "### 6. Fit-predict\n",
    "\n",
    "The `fit_predict` action trains a classifier using the training data and then uses it to obtain hard and soft predictions (probabilities).\n",
    "\n",
    "#### Inputs\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "| name | string | The name of the classifier to be used. | Yes |\n",
    "| X_train | array or list or string | The training data. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| y_train | array or list or string | The training target values. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| X_test | array or list or string | The test data. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| fit_params | dictionary | Additional parameters to pass to the `fit` method of the classifier. | No |\n",
    "| options[\"out\"] | string | The output JSON filename to save the predictions in the shared data directory. If not provided, the predictions are returned in the job result. | No |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e232c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(\n",
    "    action=\"fit_predict\",\n",
    "    name=\"classifier_name\",  # specify the name of the classifier to use here\n",
    "    X_train=X_train,  # or \"X_train.npy\" if you uploaded it in the shared data directory\n",
    "    y_train=y_train,  # or \"y_train.npy\" if you uploaded it in the shared data directory\n",
    "    X_test=X_test,  # or \"X_test.npy\" if you uploaded it in the shared data directory\n",
    "    fit_params={},  # define the fit parameters here\n",
    "    options={\n",
    "        \"out\": \"output.json\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0545459",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "\n",
    "- `name`:\n",
    "    - The name must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - A classifier with the same name should already exist in the shared data directory.\n",
    "\n",
    "- `options[\"out\"]`:\n",
    "    - The filename must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - It must have the `.json` extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cade60",
   "metadata": {},
   "source": [
    "### 7. Create-fit-predict\n",
    "\n",
    "The `create_fit_predict` action creates a classifier, trains it using the provided training data, and then uses it to obtain hard and soft predictions (probabilities).\n",
    "\n",
    "#### Inputs\n",
    "\n",
    "|     Name    |    Type     | Description |   Required  |\n",
    "|-------------|-------------|-------------|-------------|\n",
    "| action | string | The name of the action from among \"create\", \"list\", \"fit\", \"predict\", \"fit_predict\", \"create_fit_predict\" and \"delete\". | Yes |\n",
    "| name | string | The name of the classifier to be used. | Yes |\n",
    "| quantum_classifier | string | The type of the classifier, i.e., \"QuantumEnhancedEnsembleClassifier\". Default is \"QuantumEnhancedEnsembleClassifier\". | No |\n",
    "| X_train | array or list or string | The training data. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| y_train | array or list or string | The training target values. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| X_test | array or list or string | The test data. This can be a NumPy array, a list, or a string referencing a filename in the shared data directory. | Yes |\n",
    "| fit_params | dictionary | Additional parameters to pass to the `fit` method of the classifier. | No |\n",
    "| options[\"save\"] | boolean | Whether to save to trained classifier in the shared data directory. Default is `True`. | No |\n",
    "| options[\"out\"] | string | The output JSON filename to save the predictions in the shared data directory. If not provided, the predictions are returned in the job result. | No |\n",
    "\n",
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43df07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"classifier_name\",  # specify your custom name for the classifier here\n",
    "    num_learners=10,\n",
    "    regularization=0.01,\n",
    "    optimizer_options={\"simulator\": True},\n",
    "    X_train=X_train,  # or \"X_train.npy\" if you uploaded it in the shared data directory\n",
    "    y_train=y_train,  # or \"y_train.npy\" if you uploaded it in the shared data directory\n",
    "    X_test=X_test,  # or \"X_test.npy\" if you uploaded it in the shared data directory\n",
    "    fit_params={},  # define the fit parameters here\n",
    "    options={\n",
    "        \"save\": True,\n",
    "        \"out\": \"output.json\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf68e5",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "\n",
    "- `name`:\n",
    "    - If `options[\"save\"]` is set to `True`:\n",
    "        - The name must be unique, a string up to 64 characters long.\n",
    "        - It can only include alphanumeric characters and underscores.\n",
    "        - It must start with a letter and cannot end with an underscore.\n",
    "        - No classifier with the same name should already exist in the shared data directory.\n",
    "\n",
    "- `options[\"out\"]`:\n",
    "    - The filename must be unique, a string up to 64 characters long.\n",
    "    - It can only include alphanumeric characters and underscores.\n",
    "    - It must start with a letter and cannot end with an underscore.\n",
    "    - It must have the `.json` extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26472870",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73390a19",
   "metadata": {},
   "source": [
    "## Get started\n",
    "\n",
    "<LegacyContent>\n",
    "Authenticate using your [IBM Quantum Platform API token](http://quantum.ibm.com/), and select the Qiskit Function as follows:\n",
    "</LegacyContent>\n",
    "<CloudContent>\n",
    "Authenticate using your [IBM Quantum Platform API key](http://quantum.cloud.ibm.com/), and select the Qiskit Function as follows:\n",
    "</CloudContent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a715d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "\n",
    "catalog = QiskitFunctionsCatalog()\n",
    "\n",
    "# load function\n",
    "singularity = catalog.load(\"multiverse/singularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8837f5f",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In this example, you'll use the \"Singularity Machine Learning - Classification\" function to classify a dataset consisting of two interleaving, moon-shaped half-circles. The dataset is synthetic, two-dimensional, and labeled with binary labels. It is created to be challenging for algorithms such as centroid-based clustering and linear classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f9ee4",
   "metadata": {},
   "source": [
    "![Moons dataset](/images/guides/multiverse-computing-singularity/moon_shaped.avif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cedca5",
   "metadata": {},
   "source": [
    "Through this process, you'll learn how to create the classifier, fit it to the training data, use it to predict on the test data, and delete the classifier when you're finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5a40e",
   "metadata": {},
   "source": [
    "Before starting, you need to install [scikit-learn](https://scikit-learn.org/). Install it using the following command:\n",
    "\n",
    "```sh\n",
    "python3 -m pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba982810",
   "metadata": {},
   "source": [
    "Perform the following steps:\n",
    "\n",
    "1) Create the synthetic dataset using the [make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function from [scikit-learn](https://scikit-learn.org/).\n",
    "2) Upload the generated synthetic dataset to the [shared data directory](https://qiskit.github.io/qiskit-serverless/getting_started/experimental/manage_data_directory.html).\n",
    "3) Create the quantum-enhanced classifier using the [create](#2-create) action.\n",
    "4) Enlist your classifiers using the [list](#1-list) action.\n",
    "5) Train the classifier on the train data using the [fit](#4-fit) action.\n",
    "6) Use the trained classifier to predict on the test data using the [predict](#5-predict) action.\n",
    "7) Delete the classifier using the [delete](#3-delete) action.\n",
    "8) Clean up after you're done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1877f96",
   "metadata": {},
   "source": [
    "**Step 1.** Import the necessary modules and generate the synthetic dataset, then split it into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731db300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [[ 6.27427223e-01 -4.28002977e-01]\n",
      " [ 1.50566059e-01  9.88599950e-01]\n",
      " [ 6.51703130e-01  7.58474146e-01]\n",
      " [ 6.64219130e-04  4.63558306e-01]\n",
      " [ 1.80634911e+00 -9.14398615e-02]\n",
      " [ 5.57234689e-01  8.30355045e-01]\n",
      " [ 1.98286424e+00  3.15669108e-01]\n",
      " [ 7.38804205e-01 -4.65285842e-01]\n",
      " [ 1.98768342e+00  3.43534500e-01]\n",
      " [ 7.59101739e-02  1.17824656e-01]]\n",
      "Targets: [1 0 0 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# import the necessary modules for this example\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "\n",
    "# Import the make_moons and the train_test_split functions from scikit-learn\n",
    "# to create a synthetic dataset and split it into training and test datasets\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate the synthetic dataset\n",
    "X, y = make_moons(n_samples=10000)\n",
    "\n",
    "# split the data into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# print the first 10 samples of the training dataset\n",
    "print(\"Features:\", X_train[:10, :])\n",
    "print(\"Targets:\", y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f459f",
   "metadata": {},
   "source": [
    "**Step 2.** Save the labeled training and test datasets on your local disk, and then upload them to the [shared data directory](https://qiskit.github.io/qiskit-serverless/getting_started/experimental/manage_data_directory.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7265965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_test.npy.tar', 'X_train.npy.tar', 'y_test.npy.tar', 'y_train.npy.tar']\n"
     ]
    }
   ],
   "source": [
    "def make_tarfile(file_path, tar_file_name):\n",
    "    with tarfile.open(tar_file_name, \"w\") as tar:\n",
    "        tar.add(file_path, arcname=os.path.basename(file_path))\n",
    "\n",
    "\n",
    "# save the training and test datasets on your local disk\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "# create tar files for the datasets\n",
    "make_tarfile(\"X_train.npy\", \"X_train.npy.tar\")\n",
    "make_tarfile(\"y_train.npy\", \"y_train.npy.tar\")\n",
    "make_tarfile(\"X_test.npy\", \"X_test.npy.tar\")\n",
    "make_tarfile(\"y_test.npy\", \"y_test.npy.tar\")\n",
    "\n",
    "# upload the datasets to the shared data directory\n",
    "catalog.file_upload(\"X_train.npy.tar\", singularity)\n",
    "catalog.file_upload(\"y_train.npy.tar\", singularity)\n",
    "catalog.file_upload(\"X_test.npy.tar\", singularity)\n",
    "catalog.file_upload(\"y_test.npy.tar\", singularity)\n",
    "\n",
    "# view/enlist the uploaded files in the shared data directory\n",
    "print(catalog.files(singularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52caa63",
   "metadata": {},
   "source": [
    "**Step 3.** Create a quantum-enhanced classifier using the [create](#2-create) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56e1440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'message': 'Classifier created.', 'data': {}}\n"
     ]
    }
   ],
   "source": [
    "job = singularity.run(\n",
    "    action=\"create\",\n",
    "    name=\"my_classifier\",\n",
    "    num_learners=10,\n",
    "    learners_types=[\n",
    "        \"DecisionTreeClassifier\",\n",
    "        \"KNeighborsClassifier\",\n",
    "    ],\n",
    "    learners_proportions=[0.5, 0.5],\n",
    "    learners_options=[{}, {}],\n",
    "    regularization=0.01,\n",
    "    weight_update_method=\"logarithmic\",\n",
    "    sample_scaling=True,\n",
    "    optimizer_options={\"simulator\": True},\n",
    "    voting=\"soft\",\n",
    "    prob_threshold=0.5,\n",
    ")\n",
    "\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a54432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'message': 'Classifiers listed.', 'data': {'classifiers': ['my_classifier']}}\n",
      "['X_test.npy.tar', 'X_train.npy.tar', 'my_classifier.pkl.tar', 'y_test.npy.tar', 'y_train.npy.tar']\n"
     ]
    }
   ],
   "source": [
    "# list available classifiers using the list action\n",
    "job = singularity.run(action=\"list\")\n",
    "\n",
    "print(job.result())\n",
    "\n",
    "# you can also find your classifiers in the shared data directory with a *.pkl.tar extension\n",
    "print(catalog.files(singularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ddc09",
   "metadata": {},
   "source": [
    "**Step 4.** Train the quantum-enhanced classifier using the [fit](#4-fit) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13699d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'message': 'Classifier fitted.', 'data': {}}\n"
     ]
    }
   ],
   "source": [
    "job = singularity.run(\n",
    "    action=\"fit\",\n",
    "    name=\"my_classifier\",\n",
    "    X=\"X_train.npy\",  # you do not need to specify the tar extension\n",
    "    y=\"y_train.npy\",  # you do not need to specify the tar extension\n",
    ")\n",
    "\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef149b",
   "metadata": {},
   "source": [
    "**Step 5.** Obtain predictions and probabilities from the quantum-enhanced classifier using the [predict](#5-predict) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f35d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action result status:  ok\n",
      "Action result message:  Classifier predicted.\n",
      "Predictions (first five results): [1, 0, 1, 1, 1]\n",
      "Probabilities (first five results): [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "job = singularity.run(\n",
    "    action=\"predict\",\n",
    "    name=\"my_classifier\",\n",
    "    X=\"X_test.npy\",  # you do not need to specify the tar extension\n",
    ")\n",
    "\n",
    "result = job.result()\n",
    "\n",
    "print(\"Action result status: \", result[\"status\"])\n",
    "print(\"Action result message: \", result[\"message\"])\n",
    "print(\"Predictions (first five results):\", result[\"data\"][\"predictions\"][:5])\n",
    "print(\n",
    "    \"Probabilities (first five results):\", result[\"data\"][\"probabilities\"][:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980104c",
   "metadata": {},
   "source": [
    "**Step 6.** Delete the quantum-enhanced classifier using the [delete](#3-delete) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4705154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'message': 'Classifier deleted.', 'data': {}}\n"
     ]
    }
   ],
   "source": [
    "job = singularity.run(\n",
    "    action=\"delete\",\n",
    "    name=\"my_classifier\",\n",
    ")\n",
    "\n",
    "# or you can delete from the shared data directory\n",
    "# catalog.file_delete(\"my_classifier.pkl.tar\", singularity)\n",
    "\n",
    "print(job.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b34179",
   "metadata": {},
   "source": [
    "**Step 7.** Clean up local and shared data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the numpy files from your local disk\n",
    "os.remove(\"X_train.npy\")\n",
    "os.remove(\"y_train.npy\")\n",
    "os.remove(\"X_test.npy\")\n",
    "os.remove(\"y_test.npy\")\n",
    "\n",
    "# delete the tar files from your local disk\n",
    "os.remove(\"X_train.npy.tar\")\n",
    "os.remove(\"y_train.npy.tar\")\n",
    "os.remove(\"X_test.npy.tar\")\n",
    "os.remove(\"y_test.npy.tar\")\n",
    "\n",
    "# delete the tar files from the shared data\n",
    "catalog.file_delete(\"X_train.npy.tar\", singularity)\n",
    "catalog.file_delete(\"y_train.npy.tar\", singularity)\n",
    "catalog.file_delete(\"X_test.npy.tar\", singularity)\n",
    "catalog.file_delete(\"y_test.npy.tar\", singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6f2b3",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "These benchmarks show that the classifier can achieve extremely high accuracies on challenging problems. They also show that increasing the number of learners in the ensemble (number of qubits) can lead to increased accuracy.\n",
    "\n",
    "\"Classical accuracy\" refers to the accuracy obtained using corresponding classical state of the art which, in this case, is an AdaBoost classifier based on an ensemble of size 75. \"Quantum accuracy\", on the other hand, refers to the accuracy obtained using the \"Singularity Machine Learning - Classification\".\n",
    "\n",
    "| Problem | Dataset Size | Ensemble Size | Number of Qubits | Classical Accuracy | Quantum Accuracy | Improvement |\n",
    "|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n",
    "| Grid stability | 5000 examples, 12 features | 55 | 55 |  76% | 91% | 15% |\n",
    "| Grid stability | 5000 examples, 12 features | 65 | 65 |  76% | 92% | 16% |\n",
    "| Grid stability | 5000 examples, 12 features | 75 | 75 |  76% | 94% | 18% |\n",
    "| Grid stability | 5000 examples, 12 features | 85 | 85 |  76% | 94% | 18% |\n",
    "| Grid stability | 5000 examples, 12 features | 100 | 100 |  76% | 95% | 19% |\n",
    "\n",
    "----\n",
    "\n",
    "As quantum hardware evolves and scales, the implications for our quantum classifier become increasingly significant. While the number of qubits does impose limitations on the size of the ensemble that can be utilized, it does not restrict the volume of data that can be processed. This powerful capability enables the classifier to efficiently handle datasets containing millions of data points and thousands of features. Importantly, the constraints related to ensemble size can be addressed through the implementation of a large-scale version of the classifier. By leveraging an iterative outer-loop approach, the ensemble can be dynamically expanded, enhancing flexibility and overall performance. However, it's worth noting that this feature has not yet been implemented in the current version of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddff465",
   "metadata": {},
   "source": [
    "## Get support\n",
    "\n",
    "For any questions, [reach out to Multiverse Computing](mailto:singularity@multiversecomputing.com).\n",
    "\n",
    "Be sure to include the following information:\n",
    "\n",
    "- The Qiskit Function Job ID (`job.job_id`)\n",
    "- A detailed description of the issue\n",
    "- Any relevant error messages or codes\n",
    "- Steps to reproduce the issue\n",
    "\n",
    "## Next steps\n",
    "\n",
    "<Admonition type=\"tip\" title=\"Recommendations\">\n",
    "\n",
    "- Request access to [Multiverse Computing's Singularity Machine Learning Classification function](https://quantum.ibm.com/functions).\n",
    "- Try the [Solve classification problems with Multiverse Computing's Singularity Machine Learning](https://learning.quantum.ibm.com/catalog/tutorials) tutorial.\n",
    "\n",
    "</Admonition>"
   ]
  }
 ],
 "metadata": {
  "description": "A Qiskit Function to create and train a hybrid quantum ensemble classifier from multiple learners and optimizes them for diversity and generalization with QAOA",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "title": "Singularity Machine Learning by Multiverse Computing"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

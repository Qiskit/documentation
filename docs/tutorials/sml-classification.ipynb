{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1e3ec",
   "metadata": {},
   "source": [
    "{/* cspell:ignore hyperparameters overfitting sublinear preds prec */}\n",
    "# Hybrid quantum-enhanced ensemble classification (grid stability workflow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f69b77",
   "metadata": {},
   "source": [
    "*Usage estimate: 60 minutes for each job on an Eagle r3 processor. (NOTE: This is an estimate only. Your runtime may vary.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf80006",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This tutorial demonstrates a hybrid quantum–classical workflow that enhances a classical ensemble with a quantum optimization step. Using Multiverse Computing’s “Singularity Machine Learning – Classification” (a Qiskit Function), we train a pool of conventional learners (for example, decision trees, k-NN, logistic regression) and then refine that pool with a quantum layer to improve diversity and generalization. The objective is practical: on a real grid-stability prediction task, we compare a strong classical baseline with a quantum-optimized alternative under the same data splits, so you can see where the quantum step helps and what it costs.\n",
    "\n",
    "Why this matters: selecting a good subset from many weak learners is a combinatorial problem that grows quickly with ensemble size. Classical heuristics like boosting, bagging, and stacking perform well at moderate scales but can struggle to explore large, redundant libraries of models efficiently. The function integrates quantum algorithms - specifically QAOA (and optionally VQE in other configurations) - to search that space more effectively after the classical learners are trained, increasing the chance of finding a compact, diverse subset that generalizes better.\n",
    "\n",
    "Crucially, data scale is not limited by qubits. The heavy lifting on data - preprocessing, training the learner pool, and evaluation - remains classical and can handle millions of examples. Qubits only determine the ensemble size used in the quantum selection step. This decoupling is what makes the approach viable on today’s hardware: you keep familiar `scikit-learn` workflows for data and model training while calling the quantum step through a clean action interface in Qiskit Functions.\n",
    "\n",
    "What you will do here: prepare and balance the grid-stability dataset; establish a classical AdaBoost baseline; run several quantum configurations that vary ensemble width and regularization; execute on IBM&reg; simulators or QPUs via Qiskit Serverless; and compare accuracy, precision, recall, and F1 across all runs. Along the way, you will use the function’s action pattern (`create`, `fit`, `predict`, `fit_predict`, `create_fit_predict`) and key controls:\n",
    "- Regularization types: `onsite` (λ) for direct sparsity and `alpha` for a ratio-based trade-off between interaction and onsite terms\n",
    "- Auto-regularization: set `regularization=\"auto\"` with a target selection ratio to adapt sparsity automatically\n",
    "- Optimizer options: simulator versus QPU, repetitions, classical optimizer and its options, transpilation depth, and runtime sampler/estimator settings\n",
    "\n",
    "Benchmarks in the documentation show that accuracy improves as the number of learners (qubits) increases on challenging problems, with the quantum classifier matching or exceeding a comparable classical ensemble. In this tutorial, you will reproduce the workflow end-to-end and examine when increasing ensemble width or switching to adaptive regularization yields better F1 at reasonable resource usage. The result is a grounded view of how a quantum optimization step can complement, rather than replace, classical ensemble learning in real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b94021",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before starting this tutorial, ensure you have the following packages installed in your Python environment:\n",
    "\n",
    "- `qiskit[visualization]~=2.1.0`\n",
    "- `qiskit-serverless~=0.24.0`\n",
    "- `qiskit-ibm-runtime v0.40.1`\n",
    "- `qiskit-ibm-catalog~=0.8.0`\n",
    "- `scikit-learn==1.5.2`\n",
    "- `pandas>=2.0.0,<3.0.0`\n",
    "- `imbalanced-learn~=0.12.3`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db2e559",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section, we initialize the Qiskit Serverless client and load the Singularity Machine Learning – Classification function provided by Multiverse Computing.\n",
    "With Qiskit Serverless, you can run hybrid quantum–classical workflows on IBM managed cloud infrastructure without worrying about resource management.\n",
    "You will need an IBM Quantum Platform API key and your cloud resource name (CRN) to authenticate and access Qiskit Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe7ee1-21ce-445c-b151-598cd4cf9227",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "To run this tutorial, we use a preprocessed **grid stability classification dataset** containing power system sensor readings labeled as *stable* or *unstable*.\n",
    "The dataset is publicly hosted in the Multiverse Computing fork of the Qiskit documentation repository.\n",
    "The following cell automatically creates the required folder structure and downloads both the training and test files directly into your environment using `wget`.\n",
    "If you already have these files locally, this step will safely overwrite them to ensure version consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a32efb3-a425-4c02-804b-65029ecffb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tutorial/grid_ 100%[===================>] 612.94K  --.-KB/s    in 0.1s    \n",
      "data_tutorial/grid_ 100%[===================>] 108.19K  --.-KB/s    in 0.04s   \n",
      "Dataset files downloaded:\n",
      "-rw-r--r--@ 1 sepehr.hosseini  staff   108K 31 Oct 16:38 data_tutorial/grid_stability/test.csv\n",
      "-rw-r--r--@ 1 sepehr.hosseini  staff   613K 31 Oct 16:38 data_tutorial/grid_stability/train.csv\n"
     ]
    }
   ],
   "source": [
    "## Download dataset for Grid Stability Classification\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "!mkdir -p data_tutorial/grid_stability\n",
    "\n",
    "# Download the training and test sets from the official Qiskit documentation repo\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/train.csv \\\n",
    "  https://raw.githubusercontent.com/sepehr-multiverse/documentation/main/datasets/tutorials/grid_stability/train.csv\n",
    "\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/test.csv \\\n",
    "  https://raw.githubusercontent.com/sepehr-multiverse/documentation/main/datasets/tutorials/grid_stability/test.csv\n",
    "\n",
    "# Check the files have been downloaded\n",
    "!echo \"Dataset files downloaded:\"\n",
    "!ls -lh data_tutorial/grid_stability/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aa84f-ab37-412c-b056-7043b73380fa",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "In this section, we import all Python packages and Qiskit modules used throughout the tutorial.\n",
    "These include core scientific libraries for data handling and model evaluation - such as `NumPy`, `pandas`, and `scikit-learn` - along with visualization tools and Qiskit components for running the quantum-enhanced model.\n",
    "We also import the `QiskitRuntimeService` and `QiskitFunctionsCatalog` to connect with IBM Quantum&reg; services and access the Singularity Machine Learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c654f5-8355-4f67-b79d-c2b1c29ccc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709dc1c-b380-49f1-95c7-89197aa5e147",
   "metadata": {},
   "source": [
    "### Connect to IBM Quantum and load the Singularity function\n",
    "\n",
    "Next, we authenticate with IBM Quantum services and load the Singularity Machine Learning – Classification function from the Qiskit Functions Catalog.\n",
    "The `QiskitRuntimeService` establishes a secure connection to IBM Quantum Platform using your API token and instance CRN, allowing access to quantum backends.\n",
    "The `QiskitFunctionsCatalog` is then used to retrieve the Singularity function by name (`\"multiverse/singularity\"`), enabling us to call it later for hybrid quantum–classical computation.\n",
    "If the setup is successful, you will see a confirmation message indicating that the function has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\n"
     ]
    }
   ],
   "source": [
    "IBM_TOKEN = \"\"\n",
    "IBM_INSTANCE_TEST = \"\"\n",
    "IBM_INSTANCE_QUANTUM = \"\"\n",
    "FUNCTION_NAME = \"\"\n",
    "\n",
    "service = QiskitRuntimeService(\n",
    "    token=IBM_TOKEN,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    ")\n",
    "\n",
    "backend = service.least_busy()\n",
    "catalog = QiskitFunctionsCatalog(\n",
    "    token=IBM_TOKEN,\n",
    "    instance=IBM_INSTANCE_TEST,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    ")\n",
    "singularity = catalog.load(FUNCTION_NAME)\n",
    "print(\n",
    "    \"Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6a559-118a-4aa9-874d-9c009b5da60c",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "\n",
    "Before running the main experiments, we define a few small utility functions that streamline data loading and model evaluation.\n",
    "- `load_data()` reads the input CSV files into NumPy arrays, splitting features and labels for compatibility with `scikit-learn` and quantum workflows.\n",
    "- `evaluate_predictions()` computes key performance metrics - accuracy, precision, recall, and F1-score - and optionally reports runtime if timing information is provided.\n",
    "\n",
    "These helper functions simplify repeated operations later in the notebook and ensure consistent metric reporting across both classical and quantum classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bc841e-7365-4508-b6bf-ae57db6050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load data from the given path to X and y arrays.\"\"\"\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true, start_time=None, end_time=None):\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    precision = precision_score(y_true, predictions)\n",
    "    recall = recall_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    if start_time is not None and end_time is not None:\n",
    "        print(\"Time taken (s):\", end_time - start_time)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988ee237",
   "metadata": {},
   "source": [
    "## Step 1: Map classical inputs to a quantum problem\n",
    "\n",
    "We begin by preparing the dataset for hybrid quantum–classical experimentation. The goal of this step is to convert the raw grid-stability data into balanced training, validation, and test splits that can be used consistently by both classical and quantum workflows. Maintaining identical splits ensures that later performance comparisons are fair and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c084cde-a5cf-4661-a00c-aa243c8b0e44",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "We first load the training and test CSV files, create a validation split, and balance the dataset using random over-sampling. Balancing prevents bias toward the majority class and provides a more stable learning signal for both classical and quantum ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db0e914-a8f2-4a04-bec7-c15bac8104b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: X_train_bal (5104, 12) y_train_bal (5104,) X_val (850, 12) y_val (850,) X_test (750, 12) y_test (750,)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE: int = 123\n",
    "TRAIN_PATH = \"data_tutorial/grid_stability/train.csv\"\n",
    "TEST_PATH = \"data_tutorial/grid_stability/test.csv\"\n",
    "# Load and upload the data\n",
    "X_train, y_train = load_data(TRAIN_PATH)\n",
    "X_test, y_test = load_data(TEST_PATH)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Balance the dataset through over-sampling of the positive class\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Shapes:\",\n",
    "    \"X_train_bal\",\n",
    "    X_train_bal.shape,\n",
    "    \"y_train_bal\",\n",
    "    y_train_bal.shape,\n",
    "    \"X_val\",\n",
    "    X_val.shape,\n",
    "    \"y_val\",\n",
    "    y_val.shape,\n",
    "    \"X_test\",\n",
    "    X_test.shape,\n",
    "    \"y_test\",\n",
    "    y_test.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c214fd-09a8-4fa5-ab83-b0c71ad615d4",
   "metadata": {},
   "source": [
    "### Classical baseline: AdaBoost reference\n",
    "\n",
    "Before running any quantum optimization, we train a strong classical baseline - a standard AdaBoost classifier - on the same balanced data. This provides a reproducible reference point for later comparison, helping to quantify whether quantum optimization improves generalization or efficiency beyond a well-tuned classical ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ecd608-35b4-4e58-8a16-85ef98bf024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sepehr.hosseini/Documents/Qiskit/TUTORIAL (NOT LAB)/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical AdaBoost baseline:\n",
      "Accuracy: 0.7853333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7853333333333333\n",
      "F1: 0.8797610156833457\n"
     ]
    }
   ],
   "source": [
    "# ----- Classical baseline: AdaBoost -----\n",
    "baseline = AdaBoostClassifier(n_estimators=75, random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train_bal, y_train_bal)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "print(\"Classical AdaBoost baseline:\")\n",
    "_ = evaluate_predictions(baseline_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f36e3",
   "metadata": {},
   "source": [
    "## Step 2: Optimize problem for quantum hardware execution\n",
    "\n",
    "The ensemble selection task is cast as a combinatorial optimization problem where each weak learner is a binary decision variable, and the objective balances accuracy with sparsity through a regularization term. The `QuantumEnhancedEnsembleClassifier` solves this with QAOA on IBM hardware, while still allowing simulator-based exploration. In this tutorial we configure a realistic production profile that scales to larger ensembles: we use 75 learners to mirror a strong classical baseline, set the regularization to favor compact yet accurate ensembles, and select a QAOA configuration that converges reliably within a practical time budget. The `optimizer_options` control the hybrid loop: `simulator=False` routes circuits to the selected QPU, `num_solutions` increases search breadth, and `classical_optimizer_options` (for the inner classical optimizer) govern convergence; values around 60 iterations are a good balance for quality and runtime. Runtime options - such as moderate circuit depth (`reps`) and a standard transpilation effort - help ensure robust performance across devices. The configuration below is the “best-results” profile we will use for hardware runs; you may also create a purely simulated variant by toggling `simulator=True` to dry-run the workflow without consuming QPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0d3f9-691b-449d-83ca-e6bfce2d6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured hardware optimization profile: \n",
      "  optimizer_options={'simulator': False, 'num_solutions': 100000, 'reps': 3, 'optimization_level': 3, 'num_transpiler_runs': 30, 'classical_optimizer': 'COBYLA', 'classical_optimizer_options': {'maxiter': 60}, 'estimator_options': None, 'sampler_options': None}\n"
     ]
    }
   ],
   "source": [
    "# QAOA / runtime configuration for best results on hardware\n",
    "optimizer_options = {\n",
    "    \"simulator\": False,  # set True to test locally without QPU\n",
    "    \"num_solutions\": 100_000,  # broaden search over candidate ensembles\n",
    "    \"reps\": 3,  # QAOA depth (circuit layers)\n",
    "    \"optimization_level\": 3,  # transpilation effort\n",
    "    \"num_transpiler_runs\": 30,  # explore multiple layouts\n",
    "    \"classical_optimizer\": \"COBYLA\",  # robust default for this landscape\n",
    "    \"classical_optimizer_options\": {\n",
    "        \"maxiter\": 60  # practical convergence budget\n",
    "    },\n",
    "    # You can pass backend-specific options; leaving None uses least-busy routing\n",
    "    \"estimator_options\": None,\n",
    "    \"sampler_options\": None,\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"Configured hardware optimization profile:\",\n",
    "    f\"\\n  optimizer_options={optimizer_options}\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d480b3",
   "metadata": {},
   "source": [
    "## Step 3: Execute using Qiskit primitives\n",
    "\n",
    "We now execute the full workflow using the Singularity function’s `create_fit_predict` action to train, optimize, and evaluate the `QuantumEnhancedEnsembleClassifier` end-to-end on IBM infrastructure. The function builds the ensemble, applies quantum optimization through Qiskit primitives, and returns both predictions and job metadata (including runtime and resource usage). The classical data split from Step 1 is reused for reproducibility, with validation data passed through `fit_params` so the optimization can tune hyperparameters internally while keeping the held-out test set untouched.\n",
    "\n",
    "In this step, we explore several configurations of the quantum ensemble to understand how key parameters -specifically `num_learners` and `regularization` - affect both result quality and QPU usage.\n",
    "- `num_learners` determines the ensemble width (and implicitly, the number of qubits), influencing the model’s capacity and computational cost.\n",
    "- `regularization` controls sparsity and overfitting, shaping how many learners remain active after optimization.\n",
    "\n",
    "By varying these parameters, we can see how ensemble width and regularization interact: increasing width typically improves recall and F1 but costs more QPU time, while stronger or adaptive regularization can improve generalization at roughly the same hardware footprint. The next subsections walk through three representative configurations to illustrate these effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da41d4d-f06d-4b67-a8ee-b07fc0289558",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "This configuration uses `num_learners = 30` and `regularization = 7`.\n",
    "\n",
    "- `num_learners` controls the ensemble width — effectively the number of weak learners combined and, on quantum hardware, the **number of qubits required**. A larger value expands the combinatorial search space and can improve accuracy and recall, but also increases circuit width, compilation time, and overall QPU usage.\n",
    "- `regularization` sets the penalty strength for including additional learners. With the default "onsite" regularization, higher values enforce stronger sparsity (fewer learners kept), while lower values allow more complex ensembles.\n",
    "\n",
    "This setup provides a low-cost baseline, showing how a small ensemble behaves before scaling width or tuning sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e702c986-fd7f-4ea1-a93c-8cd5ef0c4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 30\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c261c1-a3c1-42c9-a522-63f3fc01a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sepehr.hosseini/Documents/Qiskit/TUTORIAL (NOT LAB)/.venv/lib/python3.12/site-packages/qiskit_ibm_runtime/utils/json.py:385: UserWarning: Callable <IBMBackend('ibm_fez')> is not JSON serializable and will be set to None.\n",
      "  warnings.warn(f\"Callable {obj} is not JSON serializable and will be set to None.\")\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_1 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "status_1 = job_1.status()\n",
    "result_1 = job_1.result()\n",
    "print(\"\\nQuantum job status:\", status_1)\n",
    "print(\"Action status:\", result_1.get(\"status\"))\n",
    "print(\"Action message:\", result_1.get(\"message\"))\n",
    "print(\"Metadata:\", result_1.get(\"metadata\"))\n",
    "qeec_pred_job_1 = np.array(result_1[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62b414-db77-45aa-bf2c-a8537fd7eba0",
   "metadata": {},
   "source": [
    "### Increase the number of learners\n",
    "\n",
    "Here we increase `num_learners` from 30 → 120 while keeping `regularization = 7`.\n",
    "\n",
    "- More learners expand the hypothesis space, allowing the model to capture more subtle patterns, which can lift F1 and recall.\n",
    "- However, wider circuits mean higher **QPU load** (more qubits to map and deeper transpilation), increasing runtime and cost.\n",
    "- The improvement in quality typically follows a *diminishing-returns curve*: early gains are significant, but they plateau as the ensemble saturates.\n",
    "\n",
    "This experiment makes the quality–cost trade-off visible — more width improves results, but at the expense of resource efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe64b0-2713-4b65-b768-10fa5d8dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 120\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a22fed-41c2-4407-88d4-02fbd32f3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_2 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "status_2 = job_2.status()\n",
    "result_2 = job_2.result()\n",
    "print(\"\\nQuantum job status:\", status_2)\n",
    "print(\"Action status:\", result_2.get(\"status\"))\n",
    "print(\"Action message:\", result_2.get(\"message\"))\n",
    "print(\"QPU Time:\", result_2.get(\"metadata\"))\n",
    "qeec_pred_job_2 = np.array(result_2[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94784a-3599-4911-ade9-4792b50c31bb",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In this configuration we keep `num_learners = 120` but use adaptive regularization to control sparsity more intelligently.\n",
    "\n",
    "- With `regularization = \"auto\"`, the optimizer automatically tunes the penalty strength rather than fixing it to a constant value. It searches for a value that achieves a balanced subset of learners instead of overfitting to the training data.\n",
    "- `regularization_type = \"alpha\"` defines how the penalty is applied. The alpha mode blends penalties on individual learners (onsite) with penalties on their pairwise interactions, encouraging smoother and more stable sparsity patterns.\n",
    "- `regularization_ratio ≈ 0.82` sets the target proportion of learners to keep active after regularization. Here, around 82% of the learners are expected to remain, trimming the weakest 18% automatically.\n",
    "\n",
    "This approach typically improves generalization and stabilizes F1 by pruning redundant learners without changing the qubit width. The extra cost is mostly classical - the tuning loop that adjusts α - so QPU usage remains close to the fixed regularization run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209eb51a-e44b-4f94-8975-269ec7e3d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 120\n",
    "REGULARIZATION = \"auto\"\n",
    "REGULARIZATION_TYPE = \"alpha\"\n",
    "REGULARIZATION_RATIO = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aa9db-4427-48de-aae0-c5c1da1cb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_3 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    regularization_type=REGULARIZATION_TYPE,\n",
    "    regularization_ratio=REGULARIZATION_RATIO,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "status_3 = job_3.status()\n",
    "result_3 = job_3.result()\n",
    "print(\"\\nQuantum job status:\", status_3)\n",
    "print(\"Action status:\", result_3.get(\"status\"))\n",
    "print(\"Action message:\", result_3.get(\"message\"))\n",
    "print(\"Metadata:\", result_3.get(\"metadata\"))\n",
    "qeec_pred_job_3 = np.array(result_3[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_3, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94af2",
   "metadata": {},
   "source": [
    "## Step 4: Post-process and return result in desired classical format\n",
    "\n",
    "We now post-process outputs from both the classical and quantum runs, converting them into a consistent format for downstream evaluation. This step compares predictive quality using standard metrics - accuracy, precision, recall, and F1 - and analyzes how ensemble width (`num_learners`) and sparsity control (`regularization`) influence both performance and computational behavior.\n",
    "\n",
    "The classical AdaBoost baseline provides a compact and stable reference for small-scale learning. It performs well with limited ensembles and negligible compute overhead, reflecting the strength of traditional boosting when the hypothesis space is still tractable. The quantum configurations (`qeec_pred_job_1`, `qeec_pred_job_2`, and `qeec_pred_job_3`) extend this baseline by embedding the ensemble-selection process within a variational quantum optimization loop. This allows the system to explore exponentially large subsets of learners simultaneously in superposition, addressing the combinatorial nature of ensemble selection more efficiently as scale increases.\n",
    "\n",
    "Results show that increasing `num_learners` from 30 to 120 improves recall and F1, confirming that a wider ensemble captures richer interactions among weak learners. The gain is sublinear on current hardware - each additional learner yields smaller accuracy increments - but the underlying scaling behavior remains favorable because the quantum optimizer can search broader configuration spaces without the exponential blow-up typical of classical subset selection. Regularization introduces additional nuance: a fixed λ=7 enforces consistent sparsity and stabilizes convergence, whereas adaptive α-regularization automatically tunes sparsity based on correlations between learners. This dynamic pruning often achieves slightly higher F1 for the same qubit width, balancing model complexity and generalization.\n",
    "\n",
    "When compared directly with the AdaBoost baseline, the smallest quantum configuration (L=30) reproduces similar accuracy, validating the hybrid pipeline’s correctness. At larger widths, quantum variants - especially with auto-regularization - begin to surpass the classical baseline modestly, showing improved recall and F1 without linear growth in computational cost. These improvements do not indicate immediate "quantum advantage" but rather **scaling efficiency**: the quantum optimizer maintains tractable performance as the ensemble expands, where a classical approach would face exponential growth in subset-selection complexity.\n",
    "\n",
    "In practice:\n",
    "- Use the **classical baseline** for quick validation and benchmarking on small datasets.\n",
    "- Apply **quantum ensembles** when model width or feature complexity grows—QAOA-based search scales more gracefully in those regimes.\n",
    "- Employ **adaptive α-regularization** to maintain sparsity and generalization without increasing circuit width.\n",
    "- Monitor QPU time and depth to balance quality gains against near-term hardware constraints.\n",
    "\n",
    "Together, these experiments show that quantum-optimized ensembles complement classical methods: they reproduce baseline accuracy at small scales while offering a path to efficient scaling on larger, combinatorial learning problems. As hardware improves, these scaling advantages are expected to compound, extending the feasible size and depth of ensemble-based models beyond what is classically practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1d610-f2e2-482c-af05-a882bb380528",
   "metadata": {},
   "source": [
    "### Evaluate metrics for each configuration\n",
    "\n",
    "We now evaluate all configurations - the classical AdaBoost baseline and the three quantum ensembles - using the `evaluate_predictions` helper to compute accuracy, precision, recall, and F1 on the same test set. This comparison clarifies how quantum optimization scales relative to the classical approach: at small widths, both perform similarly; as ensembles grow, the quantum method can explore larger hypothesis spaces more efficiently. The resulting table captures these trends in a consistent, quantitative form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8cc821-8311-4c84-8f94-ffb2ec3facc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Classical baseline\n",
    "acc_b, prec_b, rec_b, f1_b = evaluate_predictions(baseline_pred, y_test)\n",
    "results.append(\n",
    "    {\n",
    "        \"Config\": \"AdaBoost (Classical)\",\n",
    "        \"Accuracy\": acc_b,\n",
    "        \"Precision\": prec_b,\n",
    "        \"Recall\": rec_b,\n",
    "        \"F1\": f1_b,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Quantum runs\n",
    "for label, preds in [\n",
    "    (\"QEEC L=30, reg=7\", qeec_pred_job_1),\n",
    "    (\"QEEC L=120, reg=7\", qeec_pred_job_2),\n",
    "    (f\"QEEC L=120, reg=auto (α={REGULARIZATION_RATIO})\", qeec_pred_job_3),\n",
    "]:\n",
    "    acc, prec, rec, f1 = evaluate_predictions(preds, y_test)\n",
    "    results.append(\n",
    "        {\n",
    "            \"Config\": label,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cecf6-2c87-4356-b9be-26f4346c194f",
   "metadata": {},
   "source": [
    "### Visualize quality trends across configurations\n",
    "\n",
    "The grouped bar chart below compares **Accuracy** and **F1** across the classical baseline and the quantum ensembles (`L=30`, `L=120`, and `L=120 auto-α`). It illustrates how accuracy stabilizes while F1 gradually improves as quantum ensemble width increases, demonstrating that the hybrid method sustains performance scaling without the exponential cost growth typical of classical subset selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15c5fb-2450-4671-9bc2-471043414df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "plt.bar(x - width / 2, df_results[\"Accuracy\"], width=width, label=\"Accuracy\")\n",
    "plt.bar(x + width / 2, df_results[\"F1\"], width=width, label=\"F1\")\n",
    "plt.xticks(x, df_results[\"Config\"], rotation=10)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Classical vs Quantum ensemble performance\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd406-5166-4eb9-8506-0eedd71e9b79",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The plot confirms the expected scaling pattern. The classical AdaBoost performs strongly for smaller ensembles but becomes increasingly costly to scale as the number of weak learners grows, because its subset-selection problem expands combinatorially. The quantum-enhanced models replicate classical accuracy at low widths and begin to surpass it as ensemble size increases, especially under adaptive α-regularization. This reflects the quantum optimizer’s ability to sample and evaluate many candidate subsets in parallel through superposition, maintaining tractable search even at higher widths. While current hardware overhead offsets some of the theoretical gains, the trend illustrates the scaling efficiency advantage of the quantum formulation. In practical terms, the classical method remains preferable for lightweight benchmarks, while quantum-enhanced ensembles become advantageous as model dimensionality and ensemble size expand, offering better trade-offs between accuracy, generalization, and computational growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27afb68-f959-4f4a-910c-0aadafb7e18e",
   "metadata": {},
   "source": [
    "## Appendix: Scaling benefits and enhancements\n",
    "\n",
    "The scalability advantage of the `QuantumEnhancedEnsembleClassifier` arises from how the ensemble-selection process maps to quantum optimization.\n",
    "Classical ensemble learning methods, such as AdaBoost or random forests, become computationally expensive as the number of weak learners increases because selecting the optimal subset is a combinatorial problem that scales exponentially.\n",
    "\n",
    "In contrast, the quantum formulation - implemented here via the Quantum Approximate Optimization Algorithm (QAOA) - can explore these exponentially large search spaces more efficiently by evaluating multiple configurations in superposition.\n",
    "As a result, the training time grows with the number of learners, allowing the model to remain efficient even at larger ensemble scales.\n",
    "\n",
    "While current hardware introduces some noise and depth limitations, this workflow demonstrates a near-term hybrid approach where classical and quantum components cooperate: the quantum optimizer provides a better initialization landscape for the classical loop, improving both convergence and final model accuracy.\n",
    "As quantum processors evolve, these scaling benefits are expected to extend to larger datasets, more learners, and deeper circuit depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41a301",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- 1. [Introduction to Qiskit Functions](/docs/guides/functions)\n",
    "- 2. [Multiverse Computing Singularity Machine Learning](/docs/guides/multiverse-computing-singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5785c",
   "metadata": {},
   "source": [
    "## Tutorial survey\n",
    "\n",
    "Please take a minute to provide feedback on this tutorial. Your insights will help us improve our content offerings and user experience.\n",
    "\n",
    "[Link to survey](https://your.feedback.ibm.com/jfe/form/SV_3BLFkNVEuh0QBWm)"
   ]
  }
 ],
 "metadata": {
  "description": "TODO - description for SEO between 50 and 160 characters",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "title": "Hybrid quantum-enhanced ensemble classification (grid stability workflow)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

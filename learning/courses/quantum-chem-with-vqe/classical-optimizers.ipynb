{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "200faf06-21e8-4078-a6af-56fb2f7ce55f",
   "metadata": {
    "tags": []
   },
   "source": [
    "{/* cspell:ignore webkitallowfullscreen allowfullscreen frameborder referrerpolicy Lipinska Pellow Jarman Tilly Ardle Expressibility Arrasmith */}\n",
    "\n",
    "# Classical Optimizers\n",
    "\n",
    "## What is an optimizer?\n",
    "\n",
    "Victoria Lipinska tells us about classical optimizers, and how they function as part of VQE.\n",
    "\n",
    "You will hear about a few example optimizers and how they perform in the presence and absence of noise.\n",
    "\n",
    "<iframe src=\"https://video.ibm.com/embed/recorded/132414916\" width=\"690\" height=\"390\"></iframe>\n",
    "\n",
    "**References**\n",
    "\n",
    "- [A Comparison of Various Classical Optimizers for a Variational Quantum Linear Solver, Pellow-Jarman, et al.](https://arxiv.org/pdf/2106.08682.pdf)\n",
    "- [The Variational Quantum Eigensolver: A review of methods and best practices, Tilly, et al.](https://arxiv.org/abs/2111.05176)\n",
    "- [Quantum computational chemistry, McArdle, et al.](https://arxiv.org/pdf/1808.10402.pdf)\n",
    "- [Barren plateaus in quantum neural network training landscapes, McClean, et al.](https://arxiv.org/pdf/1803.11173.pdf)\n",
    "- [Connecting Ansatz Expressibility to Gradient Magnitudes and Barren Plateaus, Holmes, et al.](https://journals.aps.org/prxquantum/pdf/10.1103/PRXQuantum.3.010313)\n",
    "- [Effect of barren plateaus on gradient-free optimization, Arrasmith, et al.](https://arxiv.org/pdf/2011.12245.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bb3dc",
   "metadata": {},
   "source": [
    "## Coding a classical optimizer\n",
    "\n",
    "In the previous lessons you learned to make a Hamiltonian suitable for use on a quantum computer, and learned how to make a variational circuit. You also learned that the variational circuit (or ansatz) contains parameters to be varied, and the optimal choice of parameters is what yields the lowest possible cost function or energy. Thus our problem is reduced to searching parameter space for the optimal set. Most of the work in classical optimizers has been done for us, as excellent optimizers are available from any number of sources.\n",
    "\n",
    "In this lesson you will learn\n",
    "- how classical optimizers fit into a VQE calculation\n",
    "- what classical optimizers are available from SciPy\n",
    "- what optimizers are __not__ yet available through SciPy and how to supplement in the meantime using qiskit.algorithms\n",
    "- what options are available for these optimizers and the significance for quantum computing\n",
    "\n",
    "[SciPy](https://scipy.org/) is a free, open-source Python library with packages relevant to many areas of scientific computing, optimization among them. In particular, SciPy has an optimization package that includes [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6028e7e8-7f4e-45d1-b08b-c8d4e567c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy minimizer routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87d8ee",
   "metadata": {},
   "source": [
    "This minimize function has several arguments, and in the context of quantum chemistry, they will be\n",
    "- the cost function (cost_func). This is related to the Hamiltonian, but also includes some complexities such as the determination of the expectation value using Estimator, and in the case of excited state calculations may include orthogonality conditions.\n",
    "- an initial state (x0) for the system, often the Hartree Fock state\n",
    "- other arguments, including arguments of the cost function itself\n",
    "- the method set to the classical optimizer you select\n",
    "- options for the classical optimizer (not to be confused with Session options discussed in the next section)\n",
    "\n",
    "An example syntax is shown below. We restrict our discussion here to the last two arguments.\n",
    "\n",
    "\n",
    "```minimize(\n",
    "    cost_func,\n",
    "    x0,\n",
    "    args=(ansatz, hamiltonian, estimator),\n",
    "    method=\"cobyla\",\n",
    "    options={\"maxiter\": 200})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a5146",
   "metadata": {},
   "source": [
    "SciPy has documentation on all the available minimize methods. Here are a few noteworthy examples, all of which are methods for minimizing a scalar function of one or more variables:\n",
    "- [cobyla](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html#optimize-minimize-cobyla): Optimization BY Linear Approximation (COBYLA) algorithm.\n",
    "- [slsqp](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp): Sequential Least Squares Programming (SLSQP).\n",
    "- [nelder-mead](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html#optimize-minimize-neldermead) Nelder-Mead algorithm.\n",
    "\n",
    "These, and most available classical optimization algorithms, are local minimizers, in that they use gradients to find local minima.\n",
    "These algorithms have several options in common, but with subtle differences. For example, all have the option to specify a maximum number of iterations using the 'maxiter': 200 notation from above. All have some option specifying a different stopping criterion based on function or variable values, though these criteria are slightly different from one algorithm to the next. Cobyla, for example, allows you to specify a tolerance ('tol': 0.0001, e.g.) which is the lower bound on a \"trust region\", determined using gradients. In comparison, SLSQP lets you specify a goal in the precision of the function used in the stopping criterion ('ftol'). Nelder-Mead lets you specify a tolerance in the difference between successive parameter ($x$) guesses (xatol), and/or a tolerance in the difference between successive values obtained for the cost function $f(x)$ (fatol).\n",
    "For a complete list of available algorithms and options, visit [SciPy's minimize documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)."
   ]
  }
 ],
 "metadata": {
  "description": "This lesson discusses classical optimizers: code used to classically vary parameters to be fed into the ansatz.",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "title": "Classical optimizers",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

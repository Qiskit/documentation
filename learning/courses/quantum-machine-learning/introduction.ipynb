{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de576a4-9a1a-46e9-8d6b-e2c5178d6051",
   "metadata": {
    "gloss": {
     "computational-basis-state": {
      "text": "Also known as Z-basis states, these are the states we measure when we measure in the Z (or 'computational') basis. These are the states with labels like |00〉 and |00110100〉. IBM&reg; quantum computers always measure in the Z-basis.",
      "title": "computational basis state"
     },
     "features": {
      "text": "A feature is a property of the things we're trying to learn about that we can assign a number to. If we were learning something about cats, the features might be \"height\" or \"age\" or \"propensity to consume treats\".",
      "title": "features"
     },
     "unitary": {
      "text": "A unitary operation is a reversible operation that preserves the norm (that is, makes sure our probabilities always sum to 1). [Read more](https://en.wikipedia.org/wiki/Unitary_matrix).",
      "title": "unitary"
     }
    }
   },
   "source": [
    "---\n",
    "title: Introduction\n",
    "description: An introduction to quantum machine learning that sets expectations, describes the course structure, and gives an initial example.\n",
    "---\n",
    "\n",
    "\n",
    "{/* cspell:ignore webkitallowfullscreen allowfullscreen quantumly Ewin Yunchao Srinivasan Arunachalam Kristan */}\n",
    "\n",
    "# Introduction to Quantum Machine Learning\n",
    "\n",
    "## Overview and motivation\n",
    "\n",
    "Before beginning, please complete this short [pre-course survey](https://your.feedback.ibm.com/jfe/form/SV_8wdBVMmnHcD5zg2), which is important to help improve our content offerings and user experience.\n",
    "\n",
    "Welcome to quantum machine learning!\n",
    "\n",
    "The video below will give a brief introduction that is supplemented by the text below.\n",
    "\n",
    "<IBMVideo id=\"134355930\" title=\"In this video, Chris Porter provides an overview of subspace quantum diagonalization. He describes what factors influence its efficiency and usefulness. He explains what makes it faster than other approaches. The text below gives more detail.\"/>\n",
    "\n",
    "To briefly recap and augment the video:\n",
    "\n",
    "- We have seen a problem be solved for the first time on a quantum computer, and then subsequently people find a way to do it on a classical supercomputer. This cycle of classical and quantum computing pushing each other to their limits will likely continue for a few years.\n",
    "- There are specific problems where quantum computing can have a provable advantage over classical computing, given progress in areas such as reduction of errors and the number of qubits available. But this is still a time of exploration, searching for quantum-amenable datasets and useful quantum feature maps.\n",
    "- Quantum machine learning (QML) is one of many exciting areas where quantum computing can augment or complement existing classical workflows.\n",
    "\n",
    "Machine learning (ML) applies algorithms to data sets, and so QML might plausibly include quantum mechanics in either the data or algorithmic sides, or both. All of these possibilities are potentially interesting. But we will mostly restrict ourselves to discussions of quantum algorithms applied to classical data. One reason for this is that ML problems with classical data are already so well studied and widely available. There is broad interest in solving problems that start with classical data. Another reason is the lack of QRAM. Without the ability to store large amounts of quantum data on a relatively long timescale, methods that begin with quantum data are still fairly far from applicability to industry. It is also unclear how to \"quantumly-access\" classical data in an efficient manner. Two types of ML of particular interest are supervised learning, in which you train an algorithm using a labeled data set, and unsupervised learning, in which the algorithm attempts to learn about a distribution from unlabeled samples. An unsupervised algorithm might, for example, learn how to generate new samples from the same distribution, or how to cluster the samples into groups with similar characteristics.\n",
    "\n",
    "![QML_CR_background_Sup_Unsup.avif](/learning/images/courses/quantum-machine-learning/introduction/qml-cr-background-sup-unsup.avif)\n",
    "\n",
    "The left image shows two categories of labeled data as in supervised learning. In this case, the categories are linearly separable. The right image shows clusters of data. In an unsupervised learning task, these data would not initially be labeled and the algorithm would study the distribution, perhaps looking for clusters. For the purposes of visualizing example clusters the algorithm might identify, the data points have now been labeled. A key difference between the two is that the supervised learning process starts with the data already labeled and the unsupervised process starts with unlabeled data, even if the data are labeled at the end.\n",
    "\n",
    "Those with background in machine learning will already know that many solution methods involve mapping data into higher-dimensional spaces. This is especially well-explored in the context of kernels. As a brief reminder, sometimes data may be separable into categories by a line, plane, or hyperplane (we will often simply say \"hyperplane\" for compactness), in the same number of dimensions as the data are given. This is shown in the first image above. Other times, data may not be separable by a hyperplane in those dimensions, as shown in the second image. But there can still be structure to the data that can be exploited in a mapping to higher dimensions, which then leaves the data separable in that higher-dimensional space. This is illustrated in the mapping of the 2D data with circular symmetry into the 3D space in which the data points are arranged along a paraboloid surface.\n",
    "\n",
    "![QML_CR_background_2D-3D.avif](/learning/images/courses/quantum-machine-learning/introduction/qml-cr-background-2d-3d.avif)\n",
    "\n",
    "A common goal in QML is to find a mapping from the lower-dimensional set of features into a higher-dimensional space, that effectively separates our data points so we can use the mapping to classify new data points.\n",
    "But this is not an easy task, and any discussion of the potential usefulness of quantum computing in machine learning must be accompanied by the appropriate caveats. In particular, we must address the nuance in dataset selection and the challenges in reaching utility scale. We must also shift away from trying to outperform classical ML algorithms on data that are already handled efficiently and well by classical algorithms and refocus the discussion to investigating new feature maps that could be useful.\n",
    "\n",
    "## Managing expectations\n",
    "\n",
    "Many data sets used in QML applications described in literature are “feature engineered”, meaning a dataset is selected or generated specifically to show a narrow use case in which quantum computing is useful. If this seems like cheating then we’re misunderstanding the task at hand. It is __not__ the case that some quantum feature maps enable us to solve all or many classification tasks more efficiently or scalably than classical machine learning algorithms. Rather, some quantum feature maps (not all) behave differently from classical feature maps. The task at hand is then to explore quantum circuits in the context of complex data structures. Some specific questions to address are:\n",
    "1.\tWhat quantum circuits are most likely to behave in novel ways, compared to classical alternatives?\n",
    "2.\tAre there real-world problems that involve data with properties best explored using such novel quantum circuits?\n",
    "3.\tDo these quantum circuits scale on near-term quantum computers?\n",
    "\n",
    "### Insufficient explanation\n",
    "\n",
    "One often encounters a simplified explanation of how quantum computing can be powerful. It goes something like this:\n",
    "\n",
    "Just as classical computers use bits of information, quantum computers use qubits. Given a number of bits, say 4, a classical computer can take on any one of $2^4 = 16$ possible states, whereas a quantum computer can exist in a superposition of all 16 states simultaneously, and operations can be performed on this entire superposition. In some cases, this naturally allows us to design potentially interesting learning algorithms based on mappings to higher dimensional spaces.\n",
    "\n",
    "This is a true statement, but it is inadequate, and a bit misleading as we will explain. One also sees the differences between complex and real coefficients emphasized, as in:\n",
    "\n",
    "A probabilistic classical system in which a system can be described as having certain probabilities of being in different states, can be described as follows.\n",
    "$$\n",
    "|s\\rangle = a|0000\\rangle+b|0001\\rangle+c|0010\\rangle+... a, b, c \\in \\reals\n",
    "$$\n",
    "In such a system, the coefficients $a$, $b$, $c$, and so on can only be meaningful if they are positive, real numbers. The states in quantum computers are described by probability amplitudes that can be complex numbers.\n",
    "\n",
    "$$\n",
    "|\\psi \\rangle = A|0000\\rangle+B|0001\\rangle+C|0010\\rangle+... A, B, C \\in \\mathbb{C}\n",
    "$$\n",
    "\n",
    "The above statements have been made very carefully such that they are true (many superficially similar statements are incorrect). But these correct statements are not an explanation of the power of quantum computing in machine learning. For one thing, any application of quantum computing  to machine learning will involve measurements and we cannot measure a qubit to be in multiple states at once. We can prepare a qubit in a superposition like $|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left(|0\\rangle+|1\\rangle\\right)$ but a measurement will yield either $|0\\rangle$ or $|1\\rangle$. So at a bare minimum, this story about increasing dimensionality is incomplete. Further, in the context of kernels, increased dimensions in quantum computing cannot be a sufficient condition for computational power over classical alternatives, since Gaussian kernels are infinite dimensional. There are subtleties there, in that Gaussian feature maps are only used in conjunction with the “kernel trick” that sidesteps the need to ever calculate an infinite-dimensional mapped vector. But the point remains:\n",
    "\n",
    "__High dimensionality of entangled quantum states is not exponential parallelism, and is not a sufficient condition for increased power in machine learning.__\n",
    "\n",
    "In the lessons that follow, we present workflows for incorporating quantum circuits into machine learning tasks, and we do this for the explicit purpose of facilitating exploration of the power of quantum computing. No feature map or algorithm in this course is put forth as a quick path to better machine learning results for general problems, because no such feature map or algorithm exist. Rather, we present a wide array of quantum tools to be used in exploration of useful quantum computing.\n",
    "\n",
    "### Dequantization\n",
    "\n",
    "Dequantization refers to the replacement of a given quantum algorithm with a classical one that performs similarly to a quantum algorithm for a given set of tasks, typically including scaling. By some definitions, the classical algorithm should perform only polynomially slower than the quantum algorithm.\n",
    "\n",
    "Several quantum machine learning (QML) algorithms that were initially thought to provide significant speedups over classical algorithms have been dequantized in recent years. This process of dequantization has led to important insights into the potential advantages and limitations of quantum approaches to machine learning.\n",
    "\n",
    "One of the most notable dequantization results came from Ewin Tang's [work on recommendation systems](https://arxiv.org/abs/1807.04271) Tang discovered a classical algorithm that could perform recommendation tasks at speeds previously thought to be achievable only by quantum computers. This discovery challenged the assumption that quantum algorithms had an exponential advantage for this problem. More recent work by [Shin et al.](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.6.023218) has focused on identifying conditions on the dequantizability of a variational quantum machine learning model's function class.\n",
    "\n",
    "One common approach to dequantization (though not the only trick) is through consideration of data loading overhead. That is, any quantum algorithm applied to classical data will have a step in which classical data are encoded into the quantum computer. If a quantum algorithm assumes a starting point at which quantum data are already available, then one effectively hides the time required for encoding. There are contexts in which assuming quantum data may be reasonable, but many applications of interest will start with classical data. Some dequantization cases have shown that when this encoding time is included, and when classical data loading can be accomplished efficiently, the quantum algorithm no longer outperforms its classical counterpart.\n",
    "\n",
    "Even if an algorithm cannot be dequantized, that does not mean it is more efficient or scalable than all classical algorithms. As an extreme, contrived example: imagine an algorithm to select the largest j elements from a set of size k. One could write a quantum algorithm that uses Shor’s algorithm to factor each of the k elements into prime factors, and then determine the largest elements using the prime factors. Such an algorithm likely cannot be dequantized, but is drastically less efficient than classical algorithms to accomplish the same selection of largest elements (though not the unnecessary factoring part).\n",
    "\n",
    "## Existence proof\n",
    "\n",
    "In 2021, IBM Quantum&reg; researchers Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme published a paper in Nature, [A rigorous and robust quantum speed-up in supervised machine learning.](https://www.nature.com/articles/s41567-021-01287-z) Consistent with the above caveats, a classification problem was carefully chosen for this work that is (1) known to be classically hard, and (2) suitable for quantum algorithms to show a speed-up.\n",
    "\n",
    "The paper addresses the classification of data based on discrete logarithms. To quote the paper, “For a large prime number $p$ and a generator $g$ of $\\mathbb{Z}^*_p = {1, 2, . . . , p − 1}$, it is a widely-believed conjecture that no classical algorithm can compute $\\text{log}_g(x)$ on input $x \\in \\mathbb{Z}^*_p $, in time polynomial in $n = \\lceil{\\text{log}_2(p)}\\rceil$, the number of bits needed to represent $p$.” In contrast, [Shor’s algorithm](https://epubs.siam.org/doi/10.1137/S0097539795293172) is known to solve the discrete log problem in polynomial time. This choice of problems thus simultaneously satisfies the criteria above: classical hardness (unlikely to be dequantized), and known to be suitable for quantum algorithms.\n",
    "\n",
    "Through this judicious choice of classification problem, the authors were able to show an exponential speed-up using quantum kernel methods (sketched briefly below and discussed in later lessons) that is both end-to-end and robust. Here, “end-to end” refers to the assumptions about starting with classical data; the authors in this case do include the time for data encoding. Here, “robust” refers to the fact that the data to be classified are separated by a wide margin using the quantum algorithm, such that the classification success is robust to real-world considerations like finite sampling error.\n",
    "\n",
    "All this is to say that problems do exist in which quantum kernels can yield an exponential speed-up. But the current state of the science is that such problems are selected based on observations or theoretical justification that they should be amenable to quantum algorithms. It is not realistic to expect a quantum speed-up for machine learning tasks that classical computers already do quite well.\n",
    "\n",
    "Identifying such ideal cases for the exploration of quantum utility is an enormous responsibility for learners in this course. And it is not a task that can be accomplished in a course such as this. That exploration is a task for the IBM Quantum Network as a whole, made up of researchers like yourself. This course will demonstrate QML workflows and encoding strategies so that you can begin to explore for quantum utility in your area of subject matter expertise.\n",
    "\n",
    "We hope this introduction has made a few things clear about quantum machine learning:\n",
    "1. Quantum algorithms can offer an exponential speed-up over classical algorithms for very specific problems that are classically hard, and well-suited to quantum algorithms.\n",
    "2. High dimensionality of entangled states in quantum computing matters, but it is not sufficient to simply gain an advantage over classical algorithms.\n",
    "3. Finding problems that are well-suited to quantum algorithms is an extremely difficult task, and one that will largely fall to the learners in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff490cb-1b17-4ac7-bd32-9edca1aa051e",
   "metadata": {},
   "source": [
    "## Check-in questions\n",
    "\n",
    "What makes quantum states different from classical states?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "A lot. Notably: complex coefficients, and superposition with a single copy. There are many other differences that will be discussed in future lessons, including entanglement and interference.\n",
    "\n",
    "True or False? Highly entangled quantum states enable us to solve most machine learning problems more efficiently on a quantum computer.\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "False. Most machine learning problems are solved very efficiently by classical algorithms and quantum algorithms are not likely to offer any substantial speed-up. The goal in QML is to finding datasets with features that are well-described by quantum states and/or to find mappings of data features that optimize the accuracy of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158cbee-9b6e-4b74-a449-a4cb6ef27f18",
   "metadata": {},
   "source": [
    "## Course learning goals\n",
    "\n",
    "Through completing this course, you can expect to build the following core skills and competencies. Learners will be able to:\n",
    "\n",
    "1. Explain what QML is and where quantum connects to classical machine learning.\n",
    "\n",
    "2. Apply quantum vocabulary and key terms to ML workflows.\n",
    "\n",
    "3. Identify key components of a QML workflow (various types).\n",
    "\n",
    "4. Identify different types of QML and distinguish between them.\n",
    "\n",
    "5. Implement quantum kernel methods and variational quantum classifiers using Qiskit Runtime primitives and following Qiskit patterns.\n",
    "\n",
    "6. Identify where QML is most promising and where it is not.\n",
    "\n",
    "7. Adjust an example problem to their own data set.\n",
    "\n",
    "8. Be aware of issues in QML like training time, noise, and compounding error in multiple-state readouts.\n",
    "\n",
    "9. Make recommendations for where QML might benefit their organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bf2a7-8487-482f-8fcc-95d2210bc314",
   "metadata": {},
   "source": [
    "## Course structure\n",
    "\n",
    "This course is made up of several lessons. Each lesson has several check-in questions throughout the text, so you can practice new skills or check your understanding as you go. These are not required.\n",
    "\n",
    "At the end of the course, there is a 20-item quiz. You must score at least 70% on this quiz in order to obtain your Quantum Machine Learning badge, via Credly. If you score at least 70%, your badge will be automatically emailed to you, shortly thereafter. You may only submit the quiz twice. After the first submission, you will have the opportunity to take a second try at the questions you missed. After the second submission, your score is final. See the quiz for further details.\n",
    "\n",
    "The course structure is as follows:\n",
    "\n",
    "- Lesson 1: Introduction and overview\n",
    "- Lesson 2: Recap of machine learning\n",
    "- Lesson 3: Data encoding\n",
    "- Lesson 4: Quantum kernel methods and support vector machines\n",
    "- Lesson 5: Variational quantum classifiers / neural networks\n",
    "- Exam for badge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622774e8-c23c-4610-b204-80390b639852",
   "metadata": {},
   "source": [
    "## Run your first QML code\n",
    "\n",
    "It is often helpful to see where we're going, before breaking it down into pieces, and delving into background. The code cells below carry out a simple instance of a quantum kernel method. Specifically, a single kernel matrix element is calculated. Users new to kernel methods or quantum kernels should not be intimidated by this; multiple lessons in this course will be devoted to dissecting exactly what is being done in these cells.\n",
    "\n",
    "With this code we simultaneously introduce Qiskit patterns: a framework for approaching quantum computing at the utility scale. This framework consists of four steps that are very general and can be applied to most problems (though in some workstreams, certain steps may be iterated multiple times).\n",
    "\n",
    "### Qiskit patterns:\n",
    "\n",
    "* Step 1: Map classical inputs to a quantum problem\n",
    "* Step 2: Optimize problem for quantum execution\n",
    "* Step 3: Execute using Qiskit Runtime Primitives\n",
    "* Step 4: Analyzing / post-processing\n",
    "\n",
    "In the cells below, we offer only cursory explanations of the various steps, just enough for you to find the appropriate lesson to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b85808-7f02-446e-9ef6-38369c95a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-09 10:04:28--  https://raw.githubusercontent.com/qiskit-community/prototype-quantum-kernel-training/main/data/dataset_graph7.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49405 (48K) [text/plain]\n",
      "Saving to: ‘dataset_graph7.csv.2’\n",
      "\n",
      "dataset_graph7.csv. 100%[===================>]  48.25K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-05-09 10:04:29 (1.37 MB/s) - ‘dataset_graph7.csv.2’ saved [49405/49405]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8199"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some qiskit packages required for setting up our quantum circuits.\n",
    "from qiskit.circuit import Parameter, ParameterVector, QuantumCircuit\n",
    "from qiskit.circuit.library import unitary_overlap\n",
    "\n",
    "# Import StatevectorSampler as our sampler.\n",
    "from qiskit.primitives import StatevectorSampler\n",
    "\n",
    "# Step 1: Map classical inputs to a quantum problem:\n",
    "\n",
    "# Start by getting some appropriate data. The data imported below consist of 128 rows or data points.\n",
    "# Each row has 14 columns that correspond to data features, and a 15th column with a label (+/-1).\n",
    "!wget https://raw.githubusercontent.com/qiskit-community/prototype-quantum-kernel-training/main/data/dataset_graph7.csv\n",
    "\n",
    "# Import some required packages, and write a function to pull some training data out of the csv file you got above.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_training_data():\n",
    "    \"\"\"Read the training data.\"\"\"\n",
    "    df = pd.read_csv(\"dataset_graph7.csv\", sep=\",\", header=None)\n",
    "    training_data = df.values[:20, :]\n",
    "    ind = np.argsort(training_data[:, -1])\n",
    "    X_train = training_data[ind][:, :-1]\n",
    "\n",
    "    return X_train\n",
    "\n",
    "\n",
    "# Prepare training data\n",
    "X_train = get_training_data()\n",
    "\n",
    "# Empty kernel matrix\n",
    "num_samples = np.shape(X_train)[0]\n",
    "\n",
    "# Prepare feature map for computing overlap between two data points.\n",
    "# This could be pre-built feature maps like ZZFeatureMap, or a custom quantum circuit, as shown here.\n",
    "num_features = np.shape(X_train)[1]\n",
    "num_qubits = int(num_features / 2)\n",
    "entangler_map = [[0, 2], [3, 4], [2, 5], [1, 4], [2, 3], [4, 6]]\n",
    "fm = QuantumCircuit(num_qubits)\n",
    "training_param = Parameter(\"θ\")\n",
    "feature_params = ParameterVector(\"x\", num_qubits * 2)\n",
    "fm.ry(training_param, fm.qubits)\n",
    "for cz in entangler_map:\n",
    "    fm.cz(cz[0], cz[1])\n",
    "for i in range(num_qubits):\n",
    "    fm.rz(-2 * feature_params[2 * i + 1], i)\n",
    "    fm.rx(-2 * feature_params[2 * i], i)\n",
    "\n",
    "# Pick two data points, here 14 and 19, and assign the features to the circuits as parameters.\n",
    "x1 = 14\n",
    "x2 = 19\n",
    "unitary1 = fm.assign_parameters(list(X_train[x1]) + [np.pi / 2])\n",
    "unitary2 = fm.assign_parameters(list(X_train[x2]) + [np.pi / 2])\n",
    "\n",
    "# Create the overlap circuit\n",
    "overlap_circ = unitary_overlap(unitary1, unitary2)\n",
    "overlap_circ.measure_all()\n",
    "overlap_circ.draw(\"mpl\", scale=0.6, style=\"iqp\")\n",
    "\n",
    "# Step 2: Optimize problem for quantum execution\n",
    "\n",
    "# Use Qiskit Runtime service to get the least busy backend for running on real quantum computers.\n",
    "# from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "\n",
    "# service = QiskitRuntimeService(channel=\"ibm_quantum\")\n",
    "# backend = service.least_busy(\n",
    "#    operational=True, simulator=False, min_num_qubits=overlap_circ.num_qubits\n",
    "# )\n",
    "\n",
    "# Transpile the circuits optimally for the chosen backend using a pass manager.\n",
    "# from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "# pm = generate_preset_pass_manager(optimization_level=3, backend=backend)\n",
    "# overlap_ibm = pm.run(overlap_circ)\n",
    "\n",
    "# Step 3: Execute using Qiskit Runtime Primitives\n",
    "\n",
    "# Specify the number of shots to use.\n",
    "num_shots = 10_000\n",
    "\n",
    "## Evaluate the problem using statevector-based primitives from Qiskit\n",
    "sampler = StatevectorSampler()\n",
    "counts = (\n",
    "    sampler.run([overlap_circ], shots=num_shots).result()[0].data.meas.get_int_counts()\n",
    ")\n",
    "\n",
    "# Step 4: Analyze and post-processing\n",
    "\n",
    "# Find the probability of 0.\n",
    "counts.get(0, 0.0) / num_shots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453db58e-80b2-41df-8b9c-52dc17be6174",
   "metadata": {},
   "source": [
    "Although you don't need to understand all the steps above, we should try to understand the output, so we know why we are doing this. Many processes in machine learning use inner products as part of binary classification (among other things). Quantum mechanics has an obvious connection with this, since the probabilities of measuring various states $|\\phi_i\\rangle$ are given by the inner product with an initial state $|\\psi\\rangle$ through the inner product: $P_i = |\\langle\\phi_i|\\psi\\rangle|^2$. So what we have done above is created a quantum circuit that contains the features of our two data points, and maps them into the space of a quantum vector, then estimates the inner product in that space via making measurements. This is an example of quantum kernel estimation. Note we only implemented this process for two of the data points (the 14th and 19th). If we did this for all possible pairs, we could take the output (in this case the number 0.821...) and populate a matrix of results describing the overlap between all points in the training data set. This is the \"kernel matrix\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0f015-46a3-4b6d-8afa-c188cc5aab36",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "Read the question below, think about your answer, then click the triangle to reveal the solution.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "In the process above, we calculated a kernel matrix entry for the 14th and 19th data points. What value should we obtain if we use the same data point twice, here (like 14th and 14th again)? In other words, what should be the diagonal entries in the kernel matrix? Answer this question in the absence of noise, but note that deviations from your answer are possible in the presence of noise.\n",
    "\n",
    "</summary>\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "The diagonals should be 1.0. This process should be calculating the normalized inner product of a vector with itself, which must always be one.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
